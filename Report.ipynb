{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Data Mining\n",
    "IFN645 - Data Mining Technologies and Applications\n",
    "Due date: 7th April, 2019\n",
    "\n",
    "Before answering any questions we need to import the neccessary libraries and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "df = pd.read_csv('./data.csv')\n",
    "\n",
    "# Toggle whether or not to show output.\n",
    "verbose = True\n",
    "quick_mode = True\n",
    "target_value = \"IsBadBuy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Data Selection and Distribution. \n",
    "#### 1.1 What is the proportion of cars who can be classified as a “kick”?\n",
    "The proportion of *kicks* can be calculated using the function .mean() because the data is stored binary in the column *IsBadBuy*.  12.95% of the cars are *kicks* in the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of kicks: 12.94965763333012%\n"
     ]
    }
   ],
   "source": [
    "if(verbose):\n",
    "    print(\"Percentage of kicks: \" + str(df[target_value].mean() * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Did you have to fix any data quality problems? Detail them.\n",
    "We found a bunch of data quality problems that we had to take care of.\n",
    "\n",
    "Almost all data was missing on 44 of the rows, so we decided to remove them from the dataset. The percentage of *kicks* remained 12.95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PurchaseID  PurchaseTimestamp      PurchaseDate Auction  VehYear Make  \\\n",
      "20512       20512         1279065600  14/07/2010 10:00     NaN        0  NaN   \n",
      "20513       20513         1279065600  14/07/2010 10:00     NaN        0  NaN   \n",
      "20514       20514         1279065600  14/07/2010 10:00     NaN        0  NaN   \n",
      "20515       20515         1279065600  14/07/2010 10:00     NaN        0  NaN   \n",
      "20516       20516         1279065600  14/07/2010 10:00     NaN        0  NaN   \n",
      "20676       20676         1286496000   8/10/2010 10:00     NaN        0  NaN   \n",
      "20677       20677         1286496000   8/10/2010 10:00     NaN        0  NaN   \n",
      "20678       20678         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20679       20679         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20680       20680         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20681       20681         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20682       20682         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20683       20683         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20684       20684         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20685       20685         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20686       20686         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20687       20687         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20688       20688         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20689       20689         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20690       20690         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20691       20691         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20692       20692         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20693       20693         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20694       20694         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20695       20695         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20696       20696         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20697       20697         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20698       20698         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20699       20699         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20700       20700         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20701       20701         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20702       20702         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20703       20703         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20704       20704         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20705       20705         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20706       20706         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20707       20707         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20708       20708         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20709       20709         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20710       20710         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20711       20711         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20712       20712         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20713       20713         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "20714       20714         1286928000  13/10/2010 10:00     NaN        0  NaN   \n",
      "\n",
      "      Color Transmission WheelTypeID WheelType   ...     \\\n",
      "20512   NaN          NaN         NaN       NaN   ...      \n",
      "20513   NaN          NaN         NaN       NaN   ...      \n",
      "20514   NaN          NaN         NaN       NaN   ...      \n",
      "20515   NaN          NaN         NaN       NaN   ...      \n",
      "20516   NaN          NaN         NaN       NaN   ...      \n",
      "20676   NaN          NaN         NaN       NaN   ...      \n",
      "20677   NaN          NaN         NaN       NaN   ...      \n",
      "20678   NaN          NaN         NaN       NaN   ...      \n",
      "20679   NaN          NaN         NaN       NaN   ...      \n",
      "20680   NaN          NaN         NaN       NaN   ...      \n",
      "20681   NaN          NaN         NaN       NaN   ...      \n",
      "20682   NaN          NaN         NaN       NaN   ...      \n",
      "20683   NaN          NaN         NaN       NaN   ...      \n",
      "20684   NaN          NaN         NaN       NaN   ...      \n",
      "20685   NaN          NaN         NaN       NaN   ...      \n",
      "20686   NaN          NaN         NaN       NaN   ...      \n",
      "20687   NaN          NaN         NaN       NaN   ...      \n",
      "20688   NaN          NaN         NaN       NaN   ...      \n",
      "20689   NaN          NaN         NaN       NaN   ...      \n",
      "20690   NaN          NaN         NaN       NaN   ...      \n",
      "20691   NaN          NaN         NaN       NaN   ...      \n",
      "20692   NaN          NaN         NaN       NaN   ...      \n",
      "20693   NaN          NaN         NaN       NaN   ...      \n",
      "20694   NaN          NaN         NaN       NaN   ...      \n",
      "20695   NaN          NaN         NaN       NaN   ...      \n",
      "20696   NaN          NaN         NaN       NaN   ...      \n",
      "20697   NaN          NaN         NaN       NaN   ...      \n",
      "20698   NaN          NaN         NaN       NaN   ...      \n",
      "20699   NaN          NaN         NaN       NaN   ...      \n",
      "20700   NaN          NaN         NaN       NaN   ...      \n",
      "20701   NaN          NaN         NaN       NaN   ...      \n",
      "20702   NaN          NaN         NaN       NaN   ...      \n",
      "20703   NaN          NaN         NaN       NaN   ...      \n",
      "20704   NaN          NaN         NaN       NaN   ...      \n",
      "20705   NaN          NaN         NaN       NaN   ...      \n",
      "20706   NaN          NaN         NaN       NaN   ...      \n",
      "20707   NaN          NaN         NaN       NaN   ...      \n",
      "20708   NaN          NaN         NaN       NaN   ...      \n",
      "20709   NaN          NaN         NaN       NaN   ...      \n",
      "20710   NaN          NaN         NaN       NaN   ...      \n",
      "20711   NaN          NaN         NaN       NaN   ...      \n",
      "20712   NaN          NaN         NaN       NaN   ...      \n",
      "20713   NaN          NaN         NaN       NaN   ...      \n",
      "20714   NaN          NaN         NaN       NaN   ...      \n",
      "\n",
      "       MMRCurrentRetailCleanPrice MMRCurrentRetailRatio PRIMEUNIT AUCGUART  \\\n",
      "20512                         NaN                   NaN       NaN      NaN   \n",
      "20513                         NaN                   NaN       NaN      NaN   \n",
      "20514                         NaN                   NaN       NaN      NaN   \n",
      "20515                         NaN                   NaN       NaN      NaN   \n",
      "20516                         NaN                   NaN       NaN      NaN   \n",
      "20676                         NaN                   NaN       NaN      NaN   \n",
      "20677                         NaN                   NaN       NaN      NaN   \n",
      "20678                         NaN                   NaN       NaN      NaN   \n",
      "20679                         NaN                   NaN       NaN      NaN   \n",
      "20680                         NaN                   NaN       NaN      NaN   \n",
      "20681                         NaN                   NaN       NaN      NaN   \n",
      "20682                         NaN                   NaN       NaN      NaN   \n",
      "20683                         NaN                   NaN       NaN      NaN   \n",
      "20684                         NaN                   NaN       NaN      NaN   \n",
      "20685                         NaN                   NaN       NaN      NaN   \n",
      "20686                         NaN                   NaN       NaN      NaN   \n",
      "20687                         NaN                   NaN       NaN      NaN   \n",
      "20688                         NaN                   NaN       NaN      NaN   \n",
      "20689                         NaN                   NaN       NaN      NaN   \n",
      "20690                         NaN                   NaN       NaN      NaN   \n",
      "20691                         NaN                   NaN       NaN      NaN   \n",
      "20692                         NaN                   NaN       NaN      NaN   \n",
      "20693                         NaN                   NaN       NaN      NaN   \n",
      "20694                         NaN                   NaN       NaN      NaN   \n",
      "20695                         NaN                   NaN       NaN      NaN   \n",
      "20696                         NaN                   NaN       NaN      NaN   \n",
      "20697                         NaN                   NaN       NaN      NaN   \n",
      "20698                         NaN                   NaN       NaN      NaN   \n",
      "20699                         NaN                   NaN       NaN      NaN   \n",
      "20700                         NaN                   NaN       NaN      NaN   \n",
      "20701                         NaN                   NaN       NaN      NaN   \n",
      "20702                         NaN                   NaN       NaN      NaN   \n",
      "20703                         NaN                   NaN       NaN      NaN   \n",
      "20704                         NaN                   NaN       NaN      NaN   \n",
      "20705                         NaN                   NaN       NaN      NaN   \n",
      "20706                         NaN                   NaN       NaN      NaN   \n",
      "20707                         NaN                   NaN       NaN      NaN   \n",
      "20708                         NaN                   NaN       NaN      NaN   \n",
      "20709                         NaN                   NaN       NaN      NaN   \n",
      "20710                         NaN                   NaN       NaN      NaN   \n",
      "20711                         NaN                   NaN       NaN      NaN   \n",
      "20712                         NaN                   NaN       NaN      NaN   \n",
      "20713                         NaN                   NaN       NaN      NaN   \n",
      "20714                         NaN                   NaN       NaN      NaN   \n",
      "\n",
      "      VNST VehBCost IsOnlineSale WarrantyCost ForSale IsBadBuy  \n",
      "20512  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20513  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20514  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20515  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20516  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20676  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20677  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20678  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20679  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20680  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20681  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20682  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20683  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20684  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20685  NaN      NaN          NaN          NaN     Yes        1  \n",
      "20686  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20687  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20688  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20689  NaN      NaN          NaN          NaN     Yes        1  \n",
      "20690  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20691  NaN      NaN          NaN          NaN     Yes        1  \n",
      "20692  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20693  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20694  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20695  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20696  NaN      NaN          NaN          NaN     Yes        1  \n",
      "20697  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20698  NaN      NaN          NaN          NaN     Yes        1  \n",
      "20699  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20700  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20701  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20702  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20703  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20704  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20705  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20706  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20707  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20708  NaN      NaN          NaN          NaN     Yes        1  \n",
      "20709  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20710  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20711  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20712  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20713  NaN      NaN          NaN          NaN     Yes        0  \n",
      "20714  NaN      NaN          NaN          NaN     Yes        0  \n",
      "\n",
      "[44 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "c_name = \"VehYear\"\n",
    "df[c_name] = df[c_name].fillna(0).astype(int)\n",
    "if(verbose):\n",
    "    print(df.loc[df[c_name] == 0])\n",
    "df = df[df[c_name] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns required data cleaning. As we did not have a data/domain expert available we had to make some assumptions:\n",
    "- *Manual* and *MANUAL* is assumed to be the same in the column *Transmission*.\n",
    "- All values except 0 and 0.0 is assumed to be 1 in the column *IsOnlineSale*.\n",
    "- *IsOnlineSale* should be represented as a binary value.\n",
    "- *IsBadBuy* should be represented as binary value.\n",
    "- All *MMR...* values should be interpreted as numbers, not strings.\n",
    "- *WheelType* empty cells should be *?*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AUTO' 'MANUAL' 'Manual' '?']\n",
      "['AUTO' 'MANUAL' '?']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_name = \"Transmission\"\n",
    "if(verbose):\n",
    "    print(df[c_name].unique())\n",
    "df[c_name] = df[c_name].mask(df[c_name] == \"Manual\", \"MANUAL\")\n",
    "if(verbose):\n",
    "    print(df[c_name].unique())\n",
    "    \n",
    "c_name = \"IsOnlineSale\"\n",
    "if(verbose):\n",
    "    dg = sns.countplot(data=df, x=c_name)\n",
    "    plt.show()\n",
    "\n",
    "df[c_name] = df[c_name].mask(df[c_name] == 0.0, \"0\")\n",
    "df[c_name] = np.where(df[c_name] == \"0\", False, True)\n",
    "\n",
    "if(verbose):\n",
    "    dg = sns.countplot(data=df, x=c_name)\n",
    "    plt.show()\n",
    "    \n",
    "c_name = \"IsBadBuy\"\n",
    "if(verbose):\n",
    "    df[c_name].unique()\n",
    "df[c_name] = df[c_name].astype(bool)\n",
    "if(verbose):\n",
    "    df[c_name].unique()\n",
    "    \n",
    "     \n",
    "for c_name in [\"MMRAcquisitionAuctionAveragePrice\",\n",
    "              \"MMRAcquisitionAuctionCleanPrice\",\n",
    "              \"MMRAcquisitionRetailAveragePrice\",\n",
    "              \"MMRAcquisitonRetailCleanPrice\",\n",
    "              \"MMRCurrentAuctionAveragePrice\",\n",
    "              \"MMRCurrentAuctionCleanPrice\",\n",
    "              \"MMRCurrentRetailAveragePrice\",\n",
    "              \"MMRCurrentRetailCleanPrice\",\n",
    "              \"MMRCurrentRetailRatio\"]:\n",
    "    df[c_name] = df[c_name].mask(df[c_name] == \"?\", 0)\n",
    "    df[c_name] = df[c_name].mask(df[c_name] == \"#VALUE!\", 0)\n",
    "    df[c_name] = pd.to_numeric(df[c_name])\n",
    "    df[c_name] = df[c_name].mask(df[c_name] == 1, 0)\n",
    "    \n",
    "c_name = \"WheelType\"\n",
    "df[c_name] = df[c_name].fillna(\"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Can you identify any clear patterns by initial exploration of the data using histogram or box plot?\n",
    "The *VehYear* column, the year in which the car was made - does seems to have a influence on the *IsBadBuy* varible.  \n",
    "It is more likely that an older car is a *kick*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if(verbose):\n",
    "    ax = sns.boxplot(x=target_value, y='VehYear', data=df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 What variables did you include in the analysis and what were their roles and measurement level set? Justify your choice.\n",
    "We decided to drop the following columns:\n",
    "- PurchaseID\n",
    "- PurchaseTimestamp\n",
    "- PurchasseData\n",
    "- WheelTypeID\n",
    "- PRIMEUNIT\n",
    "- AUCGUART\n",
    "- ForSale\n",
    "\n",
    "Identification columns is not interesting. Almost all cars are marked as for sale. *PRIMEUNIT* and *AUCGUART* is missing data in most of the data entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['PurchaseID','PurchaseTimestamp','PurchaseDate','WheelTypeID','PRIMEUNIT','AUCGUART','ForSale'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 What distribution scheme did you use? What data partitioning allocation did you set? Explain your selection.\n",
    "The distribution between good and bad buys was very skewed, which made our prediction models have a heavy tendency to predict most cars as being good buys. We decided to use undersampling to counter the skewness - so the distribution between good and bay buys were equal in our traning sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Predictive Modeling Using Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# prepare data for decision tree\n",
    "df_decision = pd.get_dummies(df)\n",
    "df_decision = df_decision.dropna()\n",
    "\n",
    "# target/input split\n",
    "y = df_decision[target_value]\n",
    "X = df_decision.drop([target_value],axis = 1)\n",
    "\n",
    "# setting random state\n",
    "rs = 10\n",
    "\n",
    "# undersampling \n",
    "badBuy_indices = df_decision[df_decision[target_value] == 0].index\n",
    "\n",
    "sample_size = sum(df_decision.IsBadBuy == 1)\n",
    "\n",
    "random_indices = np.random.choice(badBuy_indices, sample_size, replace=False)\n",
    "\n",
    "Non_badBuy_sample = df_decision.loc[random_indices]\n",
    "\n",
    "Badbuys = df_decision.loc[df_decision[target_value]==1]\n",
    "\n",
    "sample_data = Badbuys.append(Non_badBuy_sample,ignore_index=True)\n",
    "X = sample_data.drop([target_value],axis = 1)\n",
    "y = sample_data[target_value]\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Build a decision tree using the default setting. \n",
    "*(a) What is the classification accuracy on training and test datasets?*  \n",
    "Full print-out provided below. 63%\n",
    "\n",
    "*(b) What is the size of tree (i.e. number of nodes)?*  \n",
    "Default settings gives a tree with 2617 nodes, tweaked has 53 nodes.\n",
    "\n",
    "*(c) How many leaves are in the tree that is selected based on the validation dataset?*  \n",
    "27 leaves in the tree with a total of 53 nodes.\n",
    "\n",
    "*(d) Which variable is used for the first split? What are the competing splits for this first split?*  \n",
    "Our tree first splits on *WheelType_?*, or in English words - weather the type of wheels on the car is known.\n",
    "Another close contender is *VehYear*, the year the vechicle was manufactured.\n",
    "\n",
    "*(e) What are the 5 important variables in building the tree?*  \n",
    "- WheelType (unknown, boolean value)\n",
    "- VehYear\n",
    "- MMRAcquisitionAuctionCleanPrice\n",
    "- MMRAcquisitionRetailAveragePrice\n",
    "- MMRAcquisitionAuctionAveragePrice\n",
    "\n",
    "*(f) Report if you see any evidence of model overfitting.*  \n",
    "There is signs of overfitting for the default settings, but not on the tweaked tree.\n",
    "\n",
    "*(g) Did changing the default setting (i.e., only focus on changing the setting of the number of splits to create a node) help improving the model? Answer the above questions on the best performing tree.*  \n",
    "Changing **min_samples_leaf** had a negative effect on the results.  \n",
    "Changing **max_depth** helped us get better result on the test data (minimized overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT SETTINGS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.62      0.64      0.63      1590\n",
      "        True       0.63      0.61      0.62      1591\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      3181\n",
      "   macro avg       0.63      0.63      0.63      3181\n",
      "weighted avg       0.63      0.63      0.63      3181\n",
      "\n",
      "Number of nodes: 2645\n",
      "\n",
      "TWEAKED SETTINGS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.71      0.68      1590\n",
      "        True       0.68      0.62      0.65      1591\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      3181\n",
      "   macro avg       0.67      0.67      0.66      3181\n",
      "weighted avg       0.67      0.67      0.66      3181\n",
      "\n",
      "Number of nodes: 53\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=rs)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "if(verbose):\n",
    "    print(\"DEFAULT SETTINGS:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Number of nodes: \" + str(model.tree_.node_count))\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=5,random_state=rs)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "if(verbose):\n",
    "    print(\"\\nTWEAKED SETTINGS:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Number of nodes: \" + str(model.tree_.node_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph.write_png(\"week3_dt_viz.png\") # saved in the following file - will return True if successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Build another decision tree tuned with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6896644657054305\n",
      "Test accuracy: 0.6629990569003458\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.72      0.68      1590\n",
      "        True       0.68      0.61      0.64      1591\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      3181\n",
      "   macro avg       0.66      0.66      0.66      3181\n",
      "weighted avg       0.66      0.66      0.66      3181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quick_mode = False\n",
    "if(quick_mode):\n",
    "    print(\"Turn off quick mode to enable gridsearch.\")\n",
    "else:\n",
    "    params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 10),\n",
    "          'min_samples_leaf': range(20, 60, 10)}\n",
    "\n",
    "    cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "    cv.fit(X_train, y_train)\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "        print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "    y_pred = cv.predict(X_test)\n",
    "    if(verbose):\n",
    "        print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(a) What is the classification accuracy on training and test datasets?*  \n",
    "The result is 69% on the training data and 66% on the test data. The full report is printed above.  \n",
    "\n",
    "*(b) What is the size of tree (i.e. number of nodes)? Is the size different from the maximal tree or the tree in the previous step? Why?*  \n",
    "It is way lower than the maximal tree - a maximal tree would not be very useful, it would be overfitting a lot. Also, our max_depth does not allow for the creation of a maximal tree. The previous tree contained 53 nodes and the optimized tree also contains 53 nodes.\n",
    "\n",
    "*(c) How many leaves are in the tree that is selected based on the validation data set?*  \n",
    "This tree contains 26 leaves (compared to 27 in the previous).\n",
    "\n",
    "*(d) Which variable is used for the first split? What are the competing splits for this first split?*  \n",
    "Same as previous tree, the type of the wheel.\n",
    "\n",
    "*(e) What are the 5 important variables in building the tree?*\n",
    "- WheelType_? (is wheel type unknown)\n",
    "- VehYear\n",
    "- MMRAcquisitionAuctionAveragePrice\n",
    "- VehOdo\n",
    "- TopThreeAmericanName_FORD (is the maker FORD)\n",
    "\n",
    "*(f) Report if you see any evidence of model overfitting.*   \n",
    "There is a slight overfitting, but very minimal.\n",
    "\n",
    "*(g) What are the parameters used? Explain your choices.*  \n",
    "We tried to test both the *gini* and *entropy* way of measuring quality of the split.  \n",
    "*Max_depth* is varied to try to improve accuracy without overfitting the model and the *min_samples_leaf* to test if internal node splitting would result in a different and more accurate tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 53\n",
      "WheelType_? : 0.5794533546423895\n",
      "VehYear : 0.1858645272704654\n",
      "MMRAcquisitionAuctionAveragePrice : 0.10895355929167352\n",
      "VehOdo : 0.029106195380610662\n",
      "TopThreeAmericanName_FORD : 0.015964135526829685\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nodes: \" + str(model.tree_.node_count))\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph.write_png(\"opti.png\") # saved in the following file - will return True if successful\n",
    "\n",
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:5]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 What is the significant difference do you see between these two decision tree models?\n",
    "*How do they compare performance-wise? Explain why those changes may have happened.*\n",
    "The performance change is insignificant. The two decision trees are almost the same.\n",
    "This is probably caused by the most important configuration in this case being *max_depth* which we adjusted before running grid search to find parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 From the better model, can you identify which cars could potential be *kicks*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Predictive Modeling Using Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['PurchaseID' 'PurchaseTimestamp' 'PurchaseDate' 'WheelTypeID' 'PRIMEUNIT'\\n 'AUCGUART' 'ForSale'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5d1505375469>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Color'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Auction'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Make'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'WheelType'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Nationality'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TopThreeAmericanName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PurchaseID'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'PurchaseTimestamp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'PurchaseDate'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'WheelTypeID'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'PRIMEUNIT'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'AUCGUART'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ForSale'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3697\u001b[1;33m                                            errors=errors)\n\u001b[0m\u001b[0;32m   3698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3143\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   4402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4403\u001b[0m                 raise KeyError(\n\u001b[1;32m-> 4404\u001b[1;33m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[0;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['PurchaseID' 'PurchaseTimestamp' 'PurchaseDate' 'WheelTypeID' 'PRIMEUNIT'\\n 'AUCGUART' 'ForSale'] not found in axis\""
     ]
    }
   ],
   "source": [
    "sample_data.info()\n",
    "sample_data.drop(['Transmission_AUTO','Transmission_MANUAL','Auction_OTHER','Auction_MANHEIM','Auction_ADESA'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "X = sample_data.drop([target_value],axis = 1)\n",
    "y = sample_data[target_value]\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 In preparation for regression, is any imputation of missing values needed for this data set? List the variables that did.\n",
    "> 509 cars need *MMR___* data  \n",
    "> 44 cars have already been removed due to missing information (year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(verbose):\n",
    "    print(\"\\nThe following columns still have missing values:\")\n",
    "nan_list = list()\n",
    "for c_name in list(sample_data):\n",
    "    if(sample_data[c_name].isnull().values.any()):\n",
    "        nan_list.append(c_name)\n",
    "        if(verbose):\n",
    "            print(c_name)\n",
    "\n",
    "sample_data = sample_data.dropna(subset=nan_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Apply transformation method(s) to the variable(s) that need it. List the variables that needed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_skewed_columns(df):\n",
    "    # setting up subplots for easier visualisation\n",
    "    f, axes = plt.subplots(3,3, figsize=(10,10), sharex=False)\n",
    "\n",
    "    sns.distplot(df['MMRAcquisitionAuctionAveragePrice'].dropna(), hist=False, ax=axes[0,0])\n",
    "    sns.distplot(df['MMRAcquisitionAuctionCleanPrice'].dropna(), hist=False, ax=axes[0,1])\n",
    "    sns.distplot(df['MMRAcquisitionRetailAveragePrice'].dropna(), hist=False, ax=axes[0,2])\n",
    "    sns.distplot(df['MMRAcquisitonRetailCleanPrice'].dropna(), hist=False, ax=axes[1,0])\n",
    "    sns.distplot(df['MMRCurrentAuctionAveragePrice'].dropna(), hist=False, ax=axes[1,1])\n",
    "    sns.distplot(df['MMRCurrentAuctionCleanPrice'].dropna(), hist=False, ax=axes[1,2])\n",
    "    sns.distplot(df['MMRCurrentRetailAveragePrice'].dropna(), hist=False, ax=axes[2,0])\n",
    "    sns.distplot(df['MMRCurrentRetailCleanPrice'].dropna(), hist=False, ax=axes[2,1])\n",
    "    sns.distplot(df['MMRCurrentRetailRatio'].dropna(), hist=False, ax=axes[2,2])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_skewed_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list columns to be transformed\n",
    "columns_to_transform = ['MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice', 'MMRAcquisitionRetailAveragePrice',\n",
    "                        'MMRAcquisitonRetailCleanPrice','MMRCurrentAuctionAveragePrice', 'MMRCurrentAuctionCleanPrice',\n",
    "                        'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice','MMRCurrentRetailRatio']\n",
    "\n",
    "# copy the dataframe\n",
    "df_log = sample_data.copy()\n",
    "\n",
    "# transform the columns with np.log\n",
    "for col in columns_to_transform:\n",
    "    df_log[col] = df_log[col].apply(lambda x: x+1)\n",
    "    df_log[col] = df_log[col].apply(np.log)\n",
    "\n",
    "# plot them again to show the distribution\n",
    "plot_skewed_columns(df_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = df_log[target_value]\n",
    "X_log = df_log.drop([target_value], axis=1)\n",
    "\n",
    "X_mat_log = X_log.as_matrix()\n",
    "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_mat_log, y_log, test_size=0.3, stratify=y_log, \n",
    "                                                                    random_state=rs)\n",
    "\n",
    "# standardise them\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log = scaler_log.fit_transform(X_train_log, y_train_log)\n",
    "X_test_log = scaler_log.transform(X_test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Build a regression model using the default regression method with all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit it to training data\n",
    "model = LogisticRegression(random_state=rs)\n",
    "model.fit(X_train_log, y_train_log)\n",
    "\n",
    "# classification report on test data\n",
    "print(\"Train accuracy:\", model.score(X_train_log, y_train_log))\n",
    "print(\"Test accuracy:\", model.score(X_test_log, y_test_log))\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test_log)\n",
    "if(verbose):\n",
    "    print(classification_report(y_test_log, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 ... Once you done it, build another one and tune it using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_log, y_train_log)\n",
    "\n",
    "# test the best model\n",
    "print(\"Train accuracy:\", cv.score(X_train_log, y_train_log))\n",
    "print(\"Test accuracy:\", cv.score(X_test_log, y_test_log))\n",
    "\n",
    "y_pred = cv.predict(X_test_log)\n",
    "print(classification_report(y_test_log, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(h) Name the regression function used.*  \n",
    "Logistic regression was used to build the model.\n",
    "\n",
    "*(i) How much was the difference in performance of two models build, default and optimal?*  \n",
    "It is slightly improved as we can see the precision on True is 0.61 on default model but 0.62 on optimal model.\n",
    "\n",
    "*(j) Show the set parameters for the best model. What are the parameters used?\n",
    "Explain your decision. What are the optimal parameters?*  \n",
    "The set parameters is based on GridSearchCV which tries different parameters from a typical range of values and pick the optimal ones. In sklearn logistic regression, regularisation is implemented in the hyperparameter C, which denotes the inverse of regularisation strength. Smaller C means stronger regularisation. The C for our best model is 1000.\n",
    "\n",
    "*(k) Report which variables are included in the regression model.*  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(l) Report the top-5 important variables (in the order) in the model.*  \n",
    "- MMRAcquisitionRetailAveragePrice\n",
    "- MMRAcquisitionAuctionAveragePrice\n",
    "- MMRCurrentRetailCleanPrice\n",
    "- MMRCurrentAuctionAveragePrice\n",
    "- MMRCurrentAuctionCleanPrice\n",
    "\n",
    "*(m) What is classification accuracy on training and test datasets?*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy:\", model.score(X_train_log, y_train_log))\n",
    "print(\"Test accuracy:\", model.score(X_test_log, y_test_log))\n",
    "print(classification_report(y_test_log, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(n) Report any sign of overfitting*\n",
    "We can see the test accuracy is lower than train accuracy due to a slight overfitting.\n",
    "\n",
    "#### 3.4 Build another regression model using the subset of inputs selected by RFE and selection by model method. Answer the followings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfe = RFECV(estimator = LogisticRegression(random_state=rs,solver = 'lbfgs'), cv=10)\n",
    "rfe.fit(X_train_log, y_train_log) # run the RFECV\n",
    "\n",
    "\n",
    "\n",
    "X_train_sel = rfe.transform(X_train_log)\n",
    "X_test_sel = rfe.transform(X_test_log)\n",
    "\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_sel, y_train_log)\n",
    "\n",
    "# test the best model\n",
    "print(\"Train accuracy:\", cv.score(X_train_sel, y_train_log))\n",
    "print(\"Test accuracy:\", cv.score(X_test_sel, y_test_log))\n",
    "\n",
    "y_pred = cv.predict(X_test_sel)\n",
    "print(classification_report(y_test_log, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(a) Report which variables are included in the regression model.*\n",
    "*(b) Report the top-5 important variables (in the order) in the model.*\n",
    "c. What are the parameters used? Explain your choices. What are the optimal\n",
    "parameters? Which regression function is being used?\n",
    "d. Report any sign of overfitting.\n",
    "e. What is classification accuracy on training and test datasets?\n",
    "f. Did it improve/worsen the performance? Explain why those changes may\n",
    "have happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
